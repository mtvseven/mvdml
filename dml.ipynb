{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "physical-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# sklearn functions\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "\n",
    "# function to create splits for cross-fitting\n",
    "def partition (list_in, n):\n",
    "    random.shuffle(list_in)\n",
    "    return [list_in[i::n] for i in range(n)]\n",
    "\n",
    "# function to use splits for orthogonalization\n",
    "def orthog(ind, dep, indices, mod):\n",
    "    \n",
    "    # fit the model\n",
    "    modfit = mod.fit(\n",
    "        np.delete(ind, indices, 0),\n",
    "        np.delete(dep, indices, 0).ravel()\n",
    "    )\n",
    "    \n",
    "    # predict\n",
    "    dephat = modfit.predict(\n",
    "        ind[indices]\n",
    "    ).reshape(-1, 1)\n",
    "    \n",
    "    # residualize\n",
    "    depres = dep[indices] - dephat\n",
    "    \n",
    "    return depres\n",
    "\n",
    "def dml(X, y, d, ymod, dmod = None, splits = 2):\n",
    "    \n",
    "    # reshape the data if necessary\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    if len(d.shape) == 1:\n",
    "        d = d.reshape(-1, 1)\n",
    "    \n",
    "    # double model logic\n",
    "    if dmod is None:\n",
    "        dmod = ymod\n",
    "    \n",
    "    # split indices\n",
    "    I = partition(list(range(len(y))), splits)\n",
    "    \n",
    "    # initialize empty lists\n",
    "    dlist  = np.empty((0,1), float)\n",
    "    ylist  = np.empty((0,1), float)\n",
    "    thetas = np.empty((0,1), float)\n",
    "    \n",
    "    # perform cross-fitting\n",
    "    for i in I:\n",
    "        \n",
    "        # get orthogonalized treatment\n",
    "        dorth = orthog(X, d, i, dmod)\n",
    "        dlist = np.append(dlist, dorth, 0)\n",
    "        \n",
    "        # get orthogonalized response\n",
    "        yorth = orthog(X, y, i, ymod)\n",
    "        ylist = np.append(ylist, yorth, 0)\n",
    "        \n",
    "        # calculate intermediate thetas\n",
    "        thetas = np.append(thetas,\n",
    "                           np.mean(dorth*yorth)/np.mean(dorth**2))\n",
    "    \n",
    "    # prep post-orthogonalization regressors\n",
    "    D = np.hstack( (np.ones((len(dlist), 1)) , dlist) )\n",
    "    \n",
    "    # fit the DML2 model\n",
    "    coefs = np.linalg.lstsq(D, ylist, rcond = None)[0]\n",
    "    \n",
    "    # get var-cov matrix for DML2\n",
    "    res = ylist - (coefs[0]*D[0] + coefs[1]*D[1])\n",
    "    vcv = np.true_divide(1, len(y) - 2\n",
    "    )*np.dot(np.dot(res.T,res), np.linalg.inv(np.dot(D.T, D)))\n",
    "    \n",
    "    # get DML1 and DML2 coefficient\n",
    "    theta1 = np.mean(thetas)\n",
    "    theta2 = coefs[1]\n",
    "    \n",
    "    # calculate the dml1 standard error\n",
    "    se1 = np.sqrt(np.mean( (ylist - theta1*dlist)**2*dlist**2\n",
    "            ) / (np.mean(dlist**2)**2)\n",
    "        ) / np.sqrt(len(dlist) - 1)\n",
    "    \n",
    "    # calculate the dml2 standard error\n",
    "    se2 = np.sqrt(np.diagonal(vcv))[1]\n",
    "    \n",
    "    # present the output\n",
    "    return {\n",
    "        'dml1':{\n",
    "            'coef_se':np.hstack((theta1, se1))\n",
    "        },\n",
    "        'dml2':{\n",
    "            'coef_se':np.hstack((theta2, se2))\n",
    "        },\n",
    "        'orth_data':np.hstack((ylist, dlist)),\n",
    "        'indices':I\n",
    "    }\n",
    "\n",
    "def synth(\n",
    "    N = 500, K = 10, theta = 0.5,\n",
    "    seed = 1, g = None, m = None\n",
    "):\n",
    "    \n",
    "    # set the randomization seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # define \n",
    "    b = np.sin([1/K for K in range(1,(K + 1))]) + 0.01\n",
    "    sigma = make_spd_matrix(K, 1)\n",
    "    X = np.random.multivariate_normal(np.ones(K), sigma, size = [N,])\n",
    "    \n",
    "    # if no functions are supplied for \n",
    "    if g is None:\n",
    "        def g(x):\n",
    "            return np.power(np.sin(x),2)\n",
    "    if m is None:\n",
    "        def m(x,nu=0.,gamma=1.):\n",
    "            return 0.5/np.pi*(np.sinh(gamma))/(np.cosh(gamma)-np.cos(x-nu))\n",
    "    \n",
    "    # define error terms\n",
    "    e1 = np.random.standard_normal(size=[N,])\n",
    "    e2 = np.random.standard_normal(size=[N,])\n",
    "    \n",
    "    # compute the variables\n",
    "    G = g(np.dot(X,b))\n",
    "    M = m(np.dot(X,b))\n",
    "    d = M + e1\n",
    "    y = np.dot(theta,d) + G + e2\n",
    "    \n",
    "    return y, d, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "shared-phenomenon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# set monte carlo parameters\n",
    "obs_min = 100\n",
    "obs_max = 2000\n",
    "\n",
    "S_min = 2\n",
    "S_max = 100\n",
    "\n",
    "K_min = 1\n",
    "K_max = 40\n",
    "\n",
    "iters = 10000\n",
    "\n",
    "# initialize empty data frame\n",
    "dmlDF = pd.DataFrame()\n",
    "\n",
    "# set the randomization\n",
    "random.seed(0)\n",
    "\n",
    "# monte carlo simulation\n",
    "for i in range(iters):\n",
    "    \n",
    "    # select random parameters\n",
    "    n = random.randint(obs_min, obs_max)\n",
    "    s = random.randint(S_min, S_max)\n",
    "    k = random.randint(K_min, K_max)\n",
    "    \n",
    "    # create the synthetic data\n",
    "    out = synth(N = n, K = k, seed = i)\n",
    "    \n",
    "    # fit the model\n",
    "    check = dml(X = out[2], y = out[0], d = out[1], ymod = ElasticNetCV(), splits = s)\n",
    "    \n",
    "    # append results to dataframe\n",
    "    dmlDF = dmlDF.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                'N':[n],\n",
    "                'splits':[s],\n",
    "                'K':[k],\n",
    "                'dml1_theta':[check['dml1']['coef_se'][0]],\n",
    "                'dml1_se':[check['dml1']['coef_se'][1]],\n",
    "                'dml2_theta':[check['dml2']['coef_se'][0]],\n",
    "                'dml2_se':[check['dml2']['coef_se'][1]]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# split the dataframe columns to adjust and stack\n",
    "dml1DF = dmlDF[['N', 'splits', 'K', 'dml1_theta', 'dml1_se']]\n",
    "dml2DF = dmlDF[['N', 'splits', 'K', 'dml2_theta', 'dml2_se']]\n",
    "\n",
    "# add column specifying estimation method\n",
    "dml1DF['type'] = ['dml1']*dml1DF.shape[0]\n",
    "dml2DF['type'] = ['dml2']*dml2DF.shape[0]\n",
    "\n",
    "# rename the columns to be consistent\n",
    "dml1DF.columns = ['N', 'splits', 'K', 'theta', 'se', 'type']\n",
    "dml2DF.columns = ['N', 'splits', 'K', 'theta', 'se', 'type']\n",
    "\n",
    "# stack the columns\n",
    "dmlDF = pd.concat([dml1DF, dml2DF]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interesting-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in old dataframe\n",
    "dmlDF_sav = pd.read_csv('dml_monte_carlo.csv')\n",
    "\n",
    "# add new data to dataframe\n",
    "dmlDF = pd.concat([dmlDF_sav, dmlDF])\n",
    "\n",
    "# write the dataframe to a csv\n",
    "dmlDF.to_csv(\"dml_monte_carlo.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
